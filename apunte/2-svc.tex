En este primer capítulo, se resumen los elementos principales del cálculo en una variable que son necesarios para desarrollar la teoría de la optimización. Los dos algoritmos principales de minimización que se exploran son el \textbf{método de descenso} y el \textbf{método de Newton}. Siendo el primero sin duda el más utilizado en el mundo del machine learning hoy en día, aunque algunos autores han sugerido utilizar variaciones del otro (ver por ejemplo \cite{optimizacion-methods-for-dl}). En la sección \ref{svc:sec:derivada} se desarrolla el concepto intuitivo de la derivada de una función, junto con un resumen de las propiedades que esperaríamos que esta cumpliera. Luego, en la sección \ref{svc:sec:reglas} hablamos más bien de cómo realmente calcular la derivada de una función, estas ideas se enlazan con la sección anterior a través de un ejemplo de cómo las primeras dos derivadas de una función se pueden utilizar para esbozar su gráfico completo, el capítulo termina con una discusión sobre la \textbf{regla de la cadena}, concepto fundamental para lo que sigue. Después de esto, la sección \ref{svc:sec:taylor} aplica lo discutido sobre la regla de la cadena para construir los llamados \textbf{polinomios de Taylor} asociados a una función, los cuales vamos interpretar como buenas aproximaciones de la función cerca de un cierto punto $x_0$. El capítulo termina en la sección \ref{svc:sec:opti}, la cual expone los dos algoritmos principales de optimización mencionados al inicio.

\section{La derivada y sus propiedades} \label{svc:sec:derivada}

\section{Las reglas de derivación} \label{svc:sec:reglas}

\section{Polinomios de Taylor} \label{svc:sec:taylor}

\section{Algoritmos de optimización} \label{svc:sec:opti}

\section{Ejercicios}