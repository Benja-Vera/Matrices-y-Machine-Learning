\section{Sesión 1: Introducción}

Consideremos el siguiente problema ilustrativo:

\begin{tcolorbox}[title={Problema introductorio}]
    Sobre una línea recta se desplazan dos móviles $A$ y $B$ siendo observados por un observador $O$ ubicado entre ellos. El móvil $A$ inicia su viaje a 1 [m] de distancia hacia la derecha de $O$, y desplazándose a 1 [m/s] hacia la derecha, mientras que el movil $B$ inicia su viaje a $1$ [m] hacia la izquierda, desplazándose a 2 [m/s] hacia la derecha. Encuentre el instante y ubicación en los cuales los móviles se encuentran.
\end{tcolorbox}

Para plantear matemáticamente este problema, podemos considerar las distancias hacia la derecha como \textit{positivas} y hacia la izquierda como \textit{negativas}. Estableciendo así lo que en física se conoce como un \textbf{sistema de referencia}. De este modo, las ecuaciones de movimiento de los dos móviles se pueden escribir como sigue:
\begin{align*}
    x_A(t) &= 1 + t \\
    x_B(t) &= -1 + 2t \\
\end{align*}
Buscamos entonces $t^*$ tal que $x_A(t^*) = x_B(t^*) := x^*$. Estas dos incógnitas $(x^*, t^*)$ son entonces soluciones al siguiente \textbf{sistema de ecuaciones}
\begin{align*}
    x - t &= 1 \\
    x - 2t &= -1 \\
\end{align*}

Entonces, podemos definir lo siguiente:

\begin{definition}[$\R^2$, igualdad, suma, ponderación, producto matriz-vector]
Los vectores de 2 dimensiones los definimos como sigue:
\begin{itemize}
\item Definimos $\R^2$ como el conjunto de los pares ordenados de números, que en adelante denotaremos verticalmente como:
$$  \begin{pmatrix} x \\ y \end{pmatrix} \in \R^2 $$
\item Se define la igualdad entre vectores como sigue:
$$  \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} z \\ w \end{pmatrix} \iff (x = z) \wedge (y = w) $$
\item Se define la suma entre vectores como:
$$  \begin{pmatrix} x \\ y \end{pmatrix} \pm  \begin{pmatrix} z \\ w \end{pmatrix} = \begin{pmatrix} x \pm z \\ y \pm w \end{pmatrix} $$
\item También se define el producto escalar o \textit{ponderación} de vectores como:
$$  \lambda  \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} \lambda x \\ \lambda y \end{pmatrix}$$
\item Y finalmente, se define el \textit{producto matriz-vector} como:
$$  \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = x \begin{pmatrix} a \\ c \end{pmatrix} + y \begin{pmatrix} b \\ d \end{pmatrix} $$
\end{itemize}
\end{definition}

Con estas definiciones, el sistema de ecuaciones antes mencionado se puede escribir como el problema de encontrar $(x, t)$ tales que:

$$  \begin{pmatrix} 1 & -1 \\ 1 & -2  \end{pmatrix} \begin{pmatrix} x \\ t \end{pmatrix} = \begin{pmatrix} 1 \\ -1  \end{pmatrix}$$

Y esto se puede entender como un problema simple de \textit{text} una función. Es decir, para $f: A \to B$ e $y \in B$, encontrar $x \in A$ tal que $f(x) = y$. El vector al lado derecho juega el rol de $y$, y la matriz juega el rol de la función $f$. Esto nose lleva entonces a preguntarnos por las propiedades que puede tener la función $f(x) = Ax$ siendo $A$ una matriz y $x$ un vector. Esto será tema de la siguiente sesión.

\subsection{Problemas teóricos}

Antes de pasar al estudio de las propiedades del producto matriz-vector, haremos las definiciones de una manera más general. Para ello, primero comprenda las siguientes definiciones.

\begin{definition}[$\R^n$, igualdad, suma, ponderación, producto matriz-vector]
Los vectores en $n$ dimensiones los definimos como sigue:
\begin{itemize}
\item Definimos $\R^n$ como el conjunto de las $n$-tuplas ordenadas de números, las cuales denotaremos como:
$$ x = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \in  \R^n$$
\item Definimos la igualdad entre vectores en $\R^n$ como:
$$ x = y \iff  \forall i \in \{1, \dots, n\}: x_i = y_i $$
\item Definimos la suma entre vectores de $\R^n$ como:
$$ (x \pm y)_i = x_i \pm y_i $$
\item Definimos el producto escalar o \textit{ponderación} para vectores de $\R^n$ como:
$$ (\lambda x)_i = \lambda x_i $$
\item Las matrices las entendemos como \textit{bloques} de números reales, que en general no serán cuadrados sino rectangulares. Estos bloques los denotaremos como:
$$ A = \begin{pmatrix} a_{11} & \dots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \dots & a_{mn} \end{pmatrix} \in  \R^{m \times n} $$
Diremos que la matriz $A$ tiene $n$ columnas y $m$ filas. Las cuales frecuentemente querremos aislar y considerar como vectores en su propio mérito (ver la definición dada anteriormente para el producto matriz-vector). Por lo tanto, definiremos los vectores columna $A_{\bullet j} \in  \R^m$ para $j \in \{1, \dots, n\}$ por $(A_{\bullet j})_i = a_{ij}$. Además, definimos los vectores fila $A_{i \bullet} \in  \R^n$ para $i \in \{1, \dots, m\}$ por $(A_{i \bullet})_j = a_{ij}$. En otras palabras:
$$ A = \begin{pmatrix} & & \\ A_{\bullet  1} & \dots & A_{\bullet n} \\ & & \end{pmatrix} = \begin{pmatrix} & A_{1  \bullet} & \\ & \vdots & \\ & A_{m \bullet} & \end{pmatrix} $$
\item Con estas nociones, se define el producto matriz-vector para un vector $x \in  \R^n$ y una matriz $A \in  \R^{m \times n}$ por
$$ Ax = x_1 A_{\bullet  1} + \dots + x_n A_{\bullet n} = \sum_{j = 1}^n x_j A_{\bullet j} \in  \R^m$$
Es decir, $A \in  \R^{m \times n}$ convierte vectores de $\R^n$ en vectores de $\R^m$.
\end{itemize}

\end{definition}

Para asegurar que comprendemos estas definiciones, realice los siguientes ejercicios:

\begin{enumerate}
    \item Considere la siguiente matriz como ejemplo
    $$ A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9  \end{pmatrix} $$
    Encuentre:
    \begin{itemize}
        \item $a_{13}$
        \item $a_{31}$
        \item $a_{21}$
        \item $a_{12}$
        \item $A_{1  \bullet}$
        \item $A_{\bullet  1}$
        \item $A_{\bullet  3}$
        \item $A_{2  \bullet}$
    \end{itemize}
    \item Realice el siguiente cálculo de multiplicación matriz-vector:
    $$  \begin{pmatrix} 0 & 1 & 0 \\ 2 & 1 & 1 \\ 3 & 2 & 0  \end{pmatrix} \begin{pmatrix} 1 \\ 1 \\ 4  \end{pmatrix} $$
    \item \textbf{(Matrices elementales)} Realice los siguientes productos:
$$  \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1  \end{pmatrix} \begin{pmatrix} a \\b \\c\end{pmatrix}$$
$$  \begin{pmatrix} 1 & 0 & \lambda \\ 0 & 1 & 0 \\ 0 & 0 & 1  \end{pmatrix} \begin{pmatrix} a \\ b \\ c \end{pmatrix} $$
$$  \begin{pmatrix} 0 & 1 \\ 1 & 0  \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix}$$
Describa en sus palabras cómo \textit{actúa} la matriz sobre el vector en cada caso. Matrices de este tipo van a ser importantes a la hora de estudiar los sistemas de ecuaciones lineales.
\end{enumerate}

\subsection{Problemas computacionales}

Ver notebook número 4.

\section{Sesión 2: Linealidad} En base al experimento computacional realizado en la sesión anterior, en que pudimos visualizar con algunos ejemplos cómo las matrices \textit{actúan} sobre el espacio transformando los vectores sobre los cuales se aplican, pudimos observar que esta acción de las matrices sobre el espacio es en algún sentido \textit{uniforme}. Las observaciones geométricas que son relevantes de realizar son las siguientes:

\begin{itemize}
    \item El orígen está fijo bajo la transformación.
    \item La transformación convierte rectas paralelas en rectas paralelas.
    \item Las distancias entre puntos se mantienen proporcionales bajo la transformación.
\end{itemize}

Estas propiedades geométricas tienen reflejos algebráicos que se pueden resumir en la siguiente proposición.

\begin{proposition}
    Sea $A \in \R^{m \times n}$ y $x, y \in \R^n, \lambda \in \R$. Entonces, se cumple que:
    \begin{enumerate}
        \item $A 0 = 0 (\in \R^m)$
        \item $A(\lambda x) = \lambda(Ax)$
        \item $A(x + y) = Ax + Ay$
    \end{enumerate}
\end{proposition}

\begin{proof}
    Recordemos la definición de producto matriz-vector.
    \[Ax = \sum_{i = 1}^m x_i A_{\bullet i}\]
    De este modo:
    \begin{enumerate}
        \item Si $x_i = 0 \forall i \in \{1, \dots, n\}$, entonces $Ax = \sum_{i=1}^n x_i A_{\bullet i} = 0$.
        \item $A(\lambda x) = \sum_{i=1}^n \lambda x_i A_{\bullet i} \overset{(1)}{=} \lambda \sum_{i=1}^n x_i A_{\bullet i}$
        \item $A(x + y) = \sum_{i=1}^n (x+y)_i A_{\bullet i} = \sum_{i=1}^n (x_i + y_i)A_{\bullet i} = \sum_{i=1}^n \qty(x_i A_{\bullet i} + y_i A_{\bullet i}) \overset{(2)}{=} \sum_{i=1}^n x_i A_{\bullet i} + \sum_{i=1}^n y_i A_{\bullet i} = Ax + Ay$.
    \end{enumerate}
    En que se han utiliado las siguientes propiedades conocidas de las sumatorias:
    \begin{enumerate}
        \item[(1)] $\sum_{i=1}^n c b_i = c\sum_{i=1}^n b_i$
        \item[(2)] $\sum_{i=1}^n (a_i + b_i) = \sum_{i=1}^n a_i + \sum_{i=1}^n b_i$
    \end{enumerate}
\end{proof}

\begin{remark}
    La primera propiedad $A0 = 0$ puede ser vista como una consecuencia de la segunda tomando $\lambda = 0$, por lo que las propiedades fundamentales son estas últimas dos.
\end{remark}

\begin{definition}(Linealidad)
    Decimos que una función $f: \R^n \to \R^m$ es \textit{lineal} si cumple
    \begin{enumerate}
        \item $\forall x \in \R^n, \lambda \in R : f(\lambda x) = \lambda f(x)$.
        \item $\forall x, y \in \R^n : f(x + y) = f(x) + f(y)$.
    \end{enumerate}
\end{definition}

En particular, en vista de lo probado anteriormente, si $f: \R^n \to \R^m$ es del tipo $f(x) = Ax$ para alguna $A \in \R^{m \times n}$ dada, entonces $f$ es lineal. El hecho más interesante que vamos a explorar en esta sesión es que no existen más opciones que esta. Esto es consecuencia del conocido teorema que enunciamos a continuación.
\begin{theorem}(Representación Matricial)
    Si $f: \R^n \to \R^m$ es lineal, entonces existe una única $A \in \R^{m \times n}$ tal que para todo $x \in \R^n$:
    \[f(x) = Ax\]
\end{theorem}
Diremos frecuentemente que esta matriz $A$ \textit{representa} a la función $f$. Algo que es aún más interesante sobre este teorema es que la matriz $A$ se puede \textit{construir}. Es decir, no es un objeto cuya existencia está garantizada pero al que no podemos acceder explícitamente, la fórmula para obtener esta matriz la veremos en la demostración a continuación.

\begin{proof}
    Para facilidad de notación, consideremos el símbolo \textit{delta de kronecker} definido por
    \[\delta_{ij} = \begin{cases}
        1 & i = j \\
        0 & i \neq j
    \end{cases}\]
    Con esto en mente, definimos el conjunto de $n$ vectores de $\R^n$ $\{e_i\}_{i=1}^n$ que vienen definidos por $(e_i)_j = \delta_{ij}$. En otras palabras, el vector $e_i$ es aquel que tiene ceros salvo en la $i$-ésima posición en que vale $1$. Notemos que cualquier $x \in \R^n$ se puede escribir como
    \[x = \sum_{i=1}^n x_i e_i\]
    De este modo, si $f$ es lineal, entonces podemos desarrollar lo siguiente:
    \[f(x) = f\qty(\sum_{i=1}^n x_i e_i) = \sum_{i=1}^n f(x_i e_i) = \sum_{i=1}^n x_i f(e_i)\]
    Y esto puede ser visto como un producto matriz vector $Ax$ cuando (y solo cuando) definamos $A$ de modo que $A_{\bullet i} = f(e_i)$. En otras palabras, la matriz representante de $f$ tiene como columna $i$-ésima a $f(e_i)$. De este modo, se obtiene $f(x) = Ax$ y se cumple lo pedido.
\end{proof}

\begin{remark}
    La matriz representante puede ser ilustrada del siguiente modo:
    \[A = \begin{pmatrix} & & \\ f(e_1) & \dots & f(e_n) \\ & & \end{pmatrix}\]
    Notar en particular que esto significa que una función lineal viene únicamente determinada (a través de su matriz representante) por sus valores en los $n$ vectores $\{f(e_i)\}_{i=1}^n$. Esto captura la \textit{uniformidad} geométrica que se observó en la sesión anterior, ya que el valor de la función en un $x$ arbitrario puede ser obtenido en base a \textit{conectar los puntos} a partir de estos $n$ vectores.
\end{remark}

\subsection{Problemas teóricos}

\begin{enumerate}
    \item Considere la función $f: \R^2 \to \R^2$ dada por
    \[f(x, y) = (y, x)\]
    \begin{enumerate}
        \item Pruebe que $f$ es lineal.
        \item Obtenga la matriz $A \in \R^{2 \times 2}$ que representa a $f$.
    \end{enumerate}
    \item Haga lo mismo que realizó en el item anterior, pero esta vez con la función $f: \R^n \to \R$ dada por
    \[f(x_1, \dots , x_n) = x_1 + x_2 + \dots + x_n\]
\end{enumerate}

\subsection{Problemas computacionales}

\section{Sesión 3: Operaciones matriciales}

