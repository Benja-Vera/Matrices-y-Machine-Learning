\section{Sesión 1: Introducción}

Consideremos el siguiente problema ilustrativo:

\begin{tcolorbox}[title={Problema introductorio}]
    Sobre una línea recta se desplazan dos móviles $A$ y $B$ siendo observados por un observador $O$ ubicado entre ellos. El móvil $A$ inicia su viaje a 1 [m] de distancia hacia la derecha de $O$, y desplazándose a 1 [m/s] hacia la derecha, mientras que el movil $B$ inicia su viaje a $1$ [m] hacia la izquierda, desplazándose a 2 [m/s] hacia la derecha. Encuentre el instante y ubicación en los cuales los móviles se encuentran.
\end{tcolorbox}

Para plantear matemáticamente este problema, podemos considerar las distancias hacia la derecha como \textit{positivas} y hacia la izquierda como \textit{negativas}. Estableciendo así lo que en física se conoce como un \textbf{sistema de referencia}. De este modo, las ecuaciones de movimiento de los dos móviles se pueden escribir como sigue:
\begin{align*}
    x_A(t) &= 1 + t \\
    x_B(t) &= -1 + 2t \\
\end{align*}
Buscamos entonces $t^*$ tal que $x_A(t^*) = x_B(t^*) := x^*$. Estas dos incógnitas $(x^*, t^*)$ son entonces soluciones al siguiente \textbf{sistema de ecuaciones}
\begin{align*}
    x - t &= 1 \\
    x - 2t &= -1 \\
\end{align*}

Entonces, podemos definir lo siguiente:

\begin{definition}[$\R^2$, igualdad, suma, ponderación, producto matriz-vector]
Los vectores de 2 dimensiones los definimos como sigue:
\begin{itemize}
\item Definimos $\R^2$ como el conjunto de los pares ordenados de números, que en adelante denotaremos verticalmente como:
$$  \begin{pmatrix} x \\ y \end{pmatrix} \in \R^2 $$
\item Se define la igualdad entre vectores como sigue:
$$  \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} z \\ w \end{pmatrix} \iff (x = z) \wedge (y = w) $$
\item Se define la suma entre vectores como:
$$  \begin{pmatrix} x \\ y \end{pmatrix} \pm  \begin{pmatrix} z \\ w \end{pmatrix} = \begin{pmatrix} x \pm z \\ y \pm w \end{pmatrix} $$
\item También se define el producto escalar o \textit{ponderación} de vectores como:
$$  \lambda  \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} \lambda x \\ \lambda y \end{pmatrix}$$
\item Y finalmente, se define el \textit{producto matriz-vector} como:
$$  \begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = x \begin{pmatrix} a \\ c \end{pmatrix} + y \begin{pmatrix} b \\ d \end{pmatrix} $$
\end{itemize}
\end{definition}

Con estas definiciones, el sistema de ecuaciones antes mencionado se puede escribir como el problema de encontrar $(x, t)$ tales que:

$$  \begin{pmatrix} 1 & -1 \\ 1 & -2  \end{pmatrix} \begin{pmatrix} x \\ t \end{pmatrix} = \begin{pmatrix} 1 \\ -1  \end{pmatrix}$$

Y esto se puede entender como un problema simple de \textit{text} una función. Es decir, para $f: A \to B$ e $y \in B$, encontrar $x \in A$ tal que $f(x) = y$. El vector al lado derecho juega el rol de $y$, y la matriz juega el rol de la función $f$. Esto nose lleva entonces a preguntarnos por las propiedades que puede tener la función $f(x) = Ax$ siendo $A$ una matriz y $x$ un vector. Esto será tema de la siguiente sesión.

\subsection{Problemas teóricos}

Antes de pasar al estudio de las propiedades del producto matriz-vector, haremos las definiciones de una manera más general. Para ello, primero comprenda las siguientes definiciones.

\begin{definition}[$\R^n$, igualdad, suma, ponderación, producto matriz-vector]
Los vectores en $n$ dimensiones los definimos como sigue:
\begin{itemize}
\item Definimos $\R^n$ como el conjunto de las $n$-tuplas ordenadas de números, las cuales denotaremos como:
$$ x = \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} \in  \R^n$$
\item Definimos la igualdad entre vectores en $\R^n$ como:
$$ x = y \iff  \forall i \in \{1, \dots, n\}: x_i = y_i $$
\item Definimos la suma entre vectores de $\R^n$ como:
$$ (x \pm y)_i = x_i \pm y_i $$
\item Definimos el producto escalar o \textit{ponderación} para vectores de $\R^n$ como:
$$ (\lambda x)_i = \lambda x_i $$
\item Las matrices las entendemos como \textit{bloques} de números reales, que en general no serán cuadrados sino rectangulares. Estos bloques los denotaremos como:
$$ A = \begin{pmatrix} a_{11} & \dots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{m1} & \dots & a_{mn} \end{pmatrix} \in  \R^{m \times n} $$
Diremos que la matriz $A$ tiene $n$ columnas y $m$ filas. Las cuales frecuentemente querremos aislar y considerar como vectores en su propio mérito (ver la definición dada anteriormente para el producto matriz-vector). Por lo tanto, definiremos los vectores columna $A_{\bullet j} \in  \R^m$ para $j \in \{1, \dots, n\}$ por $(A_{\bullet j})_i = a_{ij}$. Además, definimos los vectores fila $A_{i \bullet} \in  \R^n$ para $i \in \{1, \dots, m\}$ por $(A_{i \bullet})_j = a_{ij}$. En otras palabras:
$$ A = \begin{pmatrix} & & \\ A_{\bullet  1} & \dots & A_{\bullet n} \\ & & \end{pmatrix} = \begin{pmatrix} & A_{1  \bullet} & \\ & \vdots & \\ & A_{m \bullet} & \end{pmatrix} $$
\item Con estas nociones, se define el producto matriz-vector para un vector $x \in  \R^n$ y una matriz $A \in  \R^{m \times n}$ por
$$ Ax = x_1 A_{\bullet  1} + \dots + x_n A_{\bullet n} = \sum_{j = 1}^n x_j A_{\bullet j} \in  \R^m$$
Es decir, $A \in  \R^{m \times n}$ convierte vectores de $\R^n$ en vectores de $\R^m$.
\end{itemize}

\end{definition}

Para asegurar que comprendemos estas definiciones, realice los siguientes ejercicios:

\begin{enumerate}
    \item Considere la siguiente matriz como ejemplo
    $$ A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9  \end{pmatrix} $$
    Encuentre:
    \begin{itemize}
        \item $a_{13}$
        \item $a_{31}$
        \item $a_{21}$
        \item $a_{12}$
        \item $A_{1  \bullet}$
        \item $A_{\bullet  1}$
        \item $A_{\bullet  3}$
        \item $A_{2  \bullet}$
    \end{itemize}
    \item Realice el siguiente cálculo de multiplicación matriz-vector:
    $$  \begin{pmatrix} 0 & 1 & 0 \\ 2 & 1 & 1 \\ 3 & 2 & 0  \end{pmatrix} \begin{pmatrix} 1 \\ 1 \\ 4  \end{pmatrix} $$
    \item \textbf{(Matrices elementales)} Realice los siguientes productos:
$$  \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1  \end{pmatrix} \begin{pmatrix} a \\b \\c\end{pmatrix}$$
$$  \begin{pmatrix} 1 & 0 & \lambda \\ 0 & 1 & 0 \\ 0 & 0 & 1  \end{pmatrix} \begin{pmatrix} a \\ b \\ c \end{pmatrix} $$
$$  \begin{pmatrix} 0 & 1 \\ 1 & 0  \end{pmatrix} \begin{pmatrix} a \\ b \end{pmatrix}$$
Describa en sus palabras cómo \textit{actúa} la matriz sobre el vector en cada caso. Matrices de este tipo van a ser importantes a la hora de estudiar los sistemas de ecuaciones lineales.
\end{enumerate}

\subsection{Problemas computacionales}

Ver notebook número 4.

\section{Sesión 2: Linealidad} En base al experimento computacional realizado en la sesión anterior, en que pudimos visualizar con algunos ejemplos cómo las matrices \textit{actúan} sobre el espacio transformando los vectores sobre los cuales se aplican, pudimos observar que esta acción de las matrices sobre el espacio es en algún sentido \textit{uniforme}. Las observaciones geométricas que son relevantes de realizar son las siguientes:

\begin{itemize}
    \item El orígen está fijo bajo la transformación.
    \item La transformación convierte rectas paralelas en rectas paralelas.
    \item Las distancias entre puntos se mantienen proporcionales bajo la transformación.
\end{itemize}

Estas propiedades geométricas tienen reflejos algebráicos que se pueden resumir en la siguiente proposición.

\begin{proposition}
    Sea $A \in \R^{m \times n}$ y $x, y \in \R^n, \lambda \in \R$. Entonces, se cumple que:
    \begin{enumerate}
        \item $A 0 = 0 (\in \R^m)$
        \item $A(\lambda x) = \lambda(Ax)$
        \item $A(x + y) = Ax + Ay$
    \end{enumerate}
\end{proposition}

\begin{proof}
    Recordemos la definición de producto matriz-vector.
    \[Ax = \sum_{i = 1}^m x_i A_{\bullet i}\]
    De este modo:
    \begin{enumerate}
        \item Si $x_i = 0 \forall i \in \{1, \dots, n\}$, entonces $Ax = \sum_{i=1}^n x_i A_{\bullet i} = 0$.
        \item $A(\lambda x) = \sum_{i=1}^n \lambda x_i A_{\bullet i} \overset{(1)}{=} \lambda \sum_{i=1}^n x_i A_{\bullet i}$
        \item $A(x + y) = \sum_{i=1}^n (x+y)_i A_{\bullet i} = \sum_{i=1}^n (x_i + y_i)A_{\bullet i} = \sum_{i=1}^n \qty(x_i A_{\bullet i} + y_i A_{\bullet i}) \overset{(2)}{=} \sum_{i=1}^n x_i A_{\bullet i} + \sum_{i=1}^n y_i A_{\bullet i} = Ax + Ay$.
    \end{enumerate}
    En que se han utiliado las siguientes propiedades conocidas de las sumatorias:
    \begin{enumerate}
        \item[(1)] $\sum_{i=1}^n c b_i = c\sum_{i=1}^n b_i$
        \item[(2)] $\sum_{i=1}^n (a_i + b_i) = \sum_{i=1}^n a_i + \sum_{i=1}^n b_i$
    \end{enumerate}
\end{proof}

\begin{remark}
    La primera propiedad $A0 = 0$ puede ser vista como una consecuencia de la segunda tomando $\lambda = 0$, por lo que las propiedades fundamentales son estas últimas dos.
\end{remark}

\begin{definition}(Linealidad)
    Decimos que una función $f: \R^n \to \R^m$ es \textit{lineal} si cumple
    \begin{enumerate}
        \item $\forall x \in \R^n, \lambda \in R : f(\lambda x) = \lambda f(x)$.
        \item $\forall x, y \in \R^n : f(x + y) = f(x) + f(y)$.
    \end{enumerate}
\end{definition}

En particular, en vista de lo probado anteriormente, si $f: \R^n \to \R^m$ es del tipo $f(x) = Ax$ para alguna $A \in \R^{m \times n}$ dada, entonces $f$ es lineal. Ejemplos de funciones lineales son las siguientes:

\begin{itemize}
    \item $f: \R \to \R$ por $f(x) = ax$ ($a \in \R$)
    \item $f: \R \to \R^2$ por $f(x) = \begin{pmatrix}
        x \\ -2x
    \end{pmatrix}$
    \item $f: \R^2 \to \R$ por $f(x, y) = 2x + y$
    \item $f: \R^2 \to \R^2$ por $f(x, y) = \begin{pmatrix}
        x + y \\ x - y
    \end{pmatrix}$
\end{itemize}

Una función no lineal es una función que no es lineal. Es decir, tal que alguna de las dos condiciones que definen la linealidad no se cumple, ya sea para algún $x \in \R^n, \lambda \in \R$ o bien para algún par $x, y \in \R^n$. Ejemplos de funciones no lineales son las siguientes:

\begin{itemize}
    \item $f: \R \to \R$ por $f(x) = x^2$
    \item $f: \R \to \R^2$ por $f(x) = \begin{pmatrix}
        1 \\ x
    \end{pmatrix}$
    \item $f: \R^2 \to \R$ por $f(x, y) = (x + y)^2$
    \item $f: \R^2 \to \R^2$ por $f(x, y) = \begin{pmatrix}
        x^2 - y^2 \\ 1 - x
    \end{pmatrix}$
\end{itemize}

Obsérvese que todos los ejemplos de funciones lineales dados, si bien no los escribimos utilizando matrices, pueden ser escritos utilizándolas. Por ejemplo:

\[f(x, y) = \begin{pmatrix}
    x + y \\ x - y
\end{pmatrix} = \begin{pmatrix}
    1 & 1 \\ 1 & -1
\end{pmatrix} \begin{pmatrix}
    x \\ y
\end{pmatrix}\]

El resultado principal de esta sesión consiste en afirmar que no existe otra opción. Es decir, que la multiplicación matricial agota todas las posibles funciones lineales. Eso nos lleva al siguiente importante teorema.

\begin{theorem}(Representación Matricial)
    Si $f: \R^n \to \R^m$ es lineal, entonces existe una única $A \in \R^{m \times n}$ tal que para todo $x \in \R^n$:
    \[f(x) = Ax\]
\end{theorem}
Diremos frecuentemente que esta matriz $A$ \textit{representa} a la función $f$. Algo que es aún más interesante sobre este teorema es que la matriz $A$ se puede \textit{construir}. Es decir, no es un objeto cuya existencia está garantizada pero al que no podemos acceder explícitamente, la fórmula para obtener esta matriz la veremos en la demostración a continuación.

\begin{proof}
    Para facilidad de notación, consideremos el símbolo \textit{delta de kronecker} definido por
    \[\delta_{ij} = \begin{cases}
        1 & i = j \\
        0 & i \neq j
    \end{cases}\]
    Con esto en mente, definimos el conjunto de $n$ vectores de $\R^n$ $\{e_i\}_{i=1}^n$ que vienen definidos por $(e_i)_j = \delta_{ij}$. En otras palabras, el vector $e_i$ es aquel que tiene ceros salvo en la $i$-ésima posición en que vale $1$. Notemos que cualquier $x \in \R^n$ se puede escribir como
    \[x = \sum_{i=1}^n x_i e_i\]
    De este modo, si $f$ es lineal, entonces podemos desarrollar lo siguiente:
    \[f(x) = f\qty(\sum_{i=1}^n x_i e_i) = \sum_{i=1}^n f(x_i e_i) = \sum_{i=1}^n x_i f(e_i)\]
    Y esto puede ser visto como un producto matriz vector $Ax$ cuando (y solo cuando) definamos $A$ de modo que $A_{\bullet i} = f(e_i)$. En otras palabras, la matriz representante de $f$ tiene como columna $i$-ésima a $f(e_i)$. De este modo, se obtiene $f(x) = Ax$ y se cumple lo pedido.
\end{proof}

\begin{remark}
    La matriz representante puede ser ilustrada del siguiente modo:
    \[A = \begin{pmatrix} & & \\ f(e_1) & \dots & f(e_n) \\ & & \end{pmatrix}\]
    Notar en particular que esto significa que una función lineal viene únicamente determinada (a través de su matriz representante) por sus valores en los $n$ vectores $\{f(e_i)\}_{i=1}^n$. Esto captura la \textit{uniformidad} geométrica que se observó en la sesión anterior, ya que el valor de la función en un $x$ arbitrario puede ser obtenido en base a \textit{conectar los puntos} a partir de estos $n$ vectores.
\end{remark}

\subsection{Problemas teóricos}

\begin{enumerate}
    \item Considere la función $f: \R^2 \to \R^2$ dada por
    \[f(x, y) = \begin{pmatrix}
        y \\ x
    \end{pmatrix}\]
    \begin{enumerate}
        \item Pruebe que $f$ es lineal.
        \item Obtenga la matriz $A \in \R^{2 \times 2}$ que representa a $f$.
    \end{enumerate}
    \item Haga lo mismo que realizó en el item anterior, pero esta vez con la función $f: \R^n \to \R$ dada por
    \[f(x_1, \dots , x_n) = x_1 + x_2 + \dots + x_n\]
    \item Consideremos $R_\theta : \R^2 \to \R^2$ tal que $R_\theta (x)$ es la rotación del vector $x$ en un ángulo $\theta$ con respecto del orígen. Se puede verificar (no lo haga, pero convénzase de que es cierto) que $\R_\theta$ es una función lineal. En consecuencia, tiene una matriz representante. Encuéntrela.
    \todo[inline]{Agregar imagen}
\end{enumerate}

\section{Sesión 3: Operaciones matriciales}

La sesión anterior nos ha convencido de que existe un vínculo estrecho entre las matrices y las funciones lineales (es más, son lo mismo). Concretemos un poco ese vínculo en términos de matemática abstracta. Consideremos el conjunto
\[\L(\R^n, \R^m) = \{f : \R^n \to \R^m : f \text{ lineal}\}\]
Y consideremos la función
\begin{align*}
    T : &\R^{m \times n} \to \L(\R^n, \R^m) \\
        & A \mapsto T(A) = f_A
\end{align*}
En que $f_A(x) = Ax$ es la función lineal asociada a la matriz $A$.

En términos que ya conocemos, el teorema de representación matricial nos dice dos cosas: Ya que la matriz representante existe, la función $T$ es \textit{epiyectiva}, y ya que es única, es \textit{inyectiva}. Entonces, la función $T$ es \textit{biyectiva} y es por lo tanto una correspondencia uno-a-uno entre las matrices y las funciones lineales. Existe entonces una \textit{función inversa} $T^{-1}: \L(\R^n, \R^m) \to \R^{m \times n}$ tal que $T^{-1}(f) = A$ tal que $T(A) = f$. Esta función inversa es precisamente la fórmula que dimos para la matriz representante. En rigor, aquí no hemos dicho nada nuevo, ya que todo el contenido está en el teorema de representación, pero es importante de todos modos darle un sentido a los términos comunes de funciones inversas.

Recordemos que sobre las funciones lineales (y en realidad sobre todas las funciones) tenemos las siguientes operaciones definidas.

\begin{definition} (Suma, resta, producto escalar, composición)
    Sean $f, g: \R^n \to \R^m$ lineales y $\lambda \in \R$. Entonces:
    \begin{enumerate}
        \item Definimos la suma y resta $f \pm g: \R^n \to \R^m$ por
        \[(f \pm g)(x) = f(x) \pm g(x)\]
        \item Definimos la ponderación escalar $\lambda f : \R^n \to \R^m$ por
        \[(\lambda f)(x) = \lambda f(x)\]
        \item Si $f: \R^n \to \R^m$ y $g: \R^m \to \R^p$ lineales, entonces definimos $g \circ f : \R^n \to \R^m$ por
        \[(g \circ f) (x) = g(f(x))\]
    \end{enumerate}
\end{definition}

Algo interesante de la linealidad es que se preserva a través de estas definiciones. Es decir, tenemos la siguiente proposición:

\begin{proposition} \label{prop:function-linearity}
    La suma, resta, ponderación y composición de funciones lineales es una función lineal.
\end{proposition}

\begin{proof}
    Queda propuesta como ejercicio.
\end{proof}

Esto significa que el conjunto $\L(\R^n, \R^m)$ tiene estructura, tiene operaciones definidas sobre sus elementos. Por lo tanto, podemos \textit{trasladar} estas operaciones hacia las matrices. Por ejemplo, nos gustaría definir el objeto $A + B$ siendo $A, B$ matrices. Y nos gustaría que esta suma tuviera alguna relación con la suma de las funciones $f_A + f_B$ asociadas. Esta función suma es lineal por la última proposición. En consecuencia, tiene una matriz representante. Nos gustaría definir esta como la \textit{suma entre las matrices} $A$ y $B$. De este modo, se cumpliría que \textit{la matriz representante de la suma es la suma de las matrices representantes}. Ya que esta suma de funciones $f_A + f_B$ solo se puede hacer si ambas tiene la forma $\R^n \to \R^m$, entonces la suma solo se puede definir para matrices de iguales tamaños.

Repitamos lo mismo esta vez con la composición: Sean $A, B$ matrices (de tamaños que vamos a descubrir prontamente). Estas dos tienen asociadas funciones $f_A$ y $f_B$ lineales, las cuales se pueden componer formando $f_A \circ f_B$. Esta composición solo tiene sentido si $f_B : \R^n \to \R^m$ y $f_A: \R^m \to \R^p$, de modo que necesitamos $B \in \R^{m \times n}$ y $A \in \R^{p \times m}$. Y definimos el \textit{producto matricial} entre las matrices $A$ y $B$ (denotado como $AB$) como aquella matriz que representa a la función lineal $f_A \circ f_B$.

Esto nos entrega la siguiente definición:

\begin{definition}(suma, resta, ponderación, producto de matrices)
    Para $A, B \in \R^{m \times n}$ y $\lambda \in \R$, definimos
    \begin{itemize}
        \item $A \pm B \in \R^{m \times n}$ como la matriz representante de la función lineal $f_A \pm f_B$.
        \item $\lambda A \in \R^{m \times n}$ como la matriz representante de la función $\lambda f_A$.
        \item Para $A \in \R^{m \times p}, B \in \R^{p \times n}$, definimos $AB \in \R^{m \times n}$ como la matriz representante de la función $f_A \circ f_B$.
    \end{itemize}
\end{definition}

Esta definición es natural desde un punto de vista teórico, pero altamente poco amigable. Ya que no nos deja calcular explícitamente ninguna de estas operaciones sin pasar por pensar en funciones y matrices representantes. Por suerte, las podemos explicitar fácilmente utilizando las fórmulas que hemos obtenido hasta el momento. Observar, por ejemplo para la suma y la ponderación, que en términos de la función $T$ introducida al inicio de la sesión, estamos definiendo\footnote{Esto esconde grandes cosas. Aplicando $T$ a ambos lados, lo que se observa es que estamos definiendo las operaciones para que se cumpla
\begin{align*}
    T(A + B) = T(A) + T(B) \\
    T(\lambda A) = \lambda T(A)
\end{align*}
Es decir, las operaciones se introducen de modo que la biyección en sí misma $T$ sea \textit{lineal}. A las funciones lineales biyectivas les llamamos \textit{isomorfismos}, y esto permite frasear lo que hemos estado haciendo de la siguiente forma: Estamos dando a $\R^{m \times n}$ la misma estructura algebraica presente en el espacio de funciones $\L(\R^n, \R^m)$ ya que el teorema de representación matricial nos decía que estaban en correspondencia uno a uno.
}
\begin{align*}
    A + B = T^{-1}(T(A) + T(B)) \\
    \lambda A = T^{-1}(\lambda T(A))
\end{align*}
De modo que aplicando la fórmula para la matriz representante, tenemos entonces que
\[A + B = \begin{pmatrix} & & \\ (f_A + f_B)(e_1) & \dots & (f_A + f_B)(e_n) \\ & & \end{pmatrix} = \begin{pmatrix} & & \\ f_A(e_1) + f_B(e_1) & \dots & f_A(e_n) + f_B(e_n) \\ & & \end{pmatrix}\]
\[= \begin{pmatrix} & & \\ A e_1 + B_{e_1} & \dots & A e_n + B e_n \\ & & \end{pmatrix} = \begin{pmatrix} & & \\ A_{\bullet 1} + B_{\bullet 1} & \dots & A_{\bullet n} + B_{\bullet n} \\ & & \end{pmatrix}\]
En términos de cada una de las entradas, esto se ve mucho más simplemente como
\[(A + B)_{ij} = A_{ij} + B_{ij}\]
Similarmente, se puede ver que
\[(\lambda A)_{ij} = \lambda A_{ij}\]
Podríamos comenzar a pensar que todo este asunto de pasar por matemática abstracta y definir a través de matrices representantes fue una pérdida de tiempo solo para definir cosas que de todos modos eran intuitivas, pero ahora consideremos el producto matricial.

De acuerdo a la definición, tenemos entonces
\[AB = \begin{pmatrix} & & \\ (f_A \circ f_B)(e_1) & \dots & (f_A \circ f_B)(e_n) \\ & & \end{pmatrix} = \begin{pmatrix} & & \\ f_A(f_B(e_1)) & \dots & f_A(f_B(e_n)) \\ & & \end{pmatrix}\]
\[= \begin{pmatrix} & & \\ f_A(B_{\bullet 1}) & \dots & f_A(B_{\bullet n}) \\ & & \end{pmatrix} = \begin{pmatrix} & & \\ AB_{\bullet 1} & \dots & A B_{\bullet n} \\ & & \end{pmatrix}\]
En otras palabras, la columna $j$-ésima del producto $AB$ es la matriz $A$ aplicada sobre el vector columna $j$-ésimo de $B$. Explícitamente:
\[(AB)_{\bullet j} = \sum_{k = 1}^{p} B_{kj} A_{\bullet k}\]
Y en términos de las entradas individuales,
\[(AB)_{ij} = \sum_{k=1}^p A_{ik} B_{kj}\]
Esta fórmula es la más conocida para el producto matricial y definitivamente no es la primera que se nos hubiese ocurrido al definirlo. Lo común en los cursos de álgebra lineal es comenzar dando estas definiciones y luego probar que \textit{mágicamente} se cumplen las cosas que ya hemos explorado (y otras propiedades que veremos más adelante), pero el camino más transparente es el que hemos hecho, en que las definiciones están al servicio de que se cumplan las propiedades.

En la siguiente sesión exploraremos el concepto de una \textit{matriz inversa} y con ello comenzaremos nuestro camino hacia el problema original de despejar $x$ en la ecuación $Ax = b$, camino que llamaremos el de \textit{invertir} una matriz $A$.

\subsection{Problemas teóricos}

\begin{enumerate}
    \item Demuestre, la proposición \ref{prop:function-linearity}.
    \item Realice los siguientes productos matriciales
        \[1\]
        \[2\]
        \[3\]
        Deduzca de los últimos dos que el producto matricial no es conmutativo.
    \item Encuentre dos matrices $A, B \in \R^{2 \times 2}$ no nulas tales que $AB = 0$.
    \item Sea $R_\theta$ la función lineal de rotación descrita en los ejercicios de la sesión anterior. Utilice el hecho intuitivo de que $R_{\alpha} \circ R_{\beta} = R_{\alpha + \beta}$ para deducir, a partir del producto matricial las identidades trigonométricas para la suma de ángulos (Identidades que pasamos varias sesiones obteniendo en la unidad anterior).
    
    A lo largo de estos siguientes problemas, vamos a utilizar nuestros resultados abstractos para demostrar de manera breve propiedades aritméticas de matrices que no son obvias y cuyas demostraciones a partir de las definiciones tradicionales resultan engorrosas, técnicas y no muy iluminadoras.
    \item Sean $f, g : \R^p \to \R^m$ y $h: \R^n \to \R^p$ funciones lineales. Pruebe que se cumple
    \[(f + g) \circ h = (f \circ h) + (g \circ h)\]
    \item  Sean $f, g : \R^n \to \R^p$ y $h: \R^p \to \R^m$ funciones lineales. Pruebe que se cumple
    \[h \circ(f + g) = (h \circ f) + (h \circ g)\]

    \begin{remark}
        Luego de hacer las demostraciones, notará que la linealidad solo se utiliza en el segundo problema y sobre $h$. Es decir, estas igualdades se cumplen en un contexto de funciones más general que aquel en el que todo es lineal. Esto es interesante, pero en este contexto solo lo utilizaremos para funciones lineales, por lo que el enunciado tiene supuestos más restrictivos.
    \end{remark}
    \item Deduzca de los problemas anteriores que las siguientes identidades matriciales se cumplen
    \[A(B + C) = AB + AC\]
    \[(A + B)C = AC + BC\]
    Siendo $A, B, C$ matrices de los tamaños que correspondan.

    \item Pruebe que el producto de matrices es \textit{asociativo}. Es decir,
    \[(AB)C = A(BC)\]
    Para ello, recuerde que la composición de funciones es evidentemente asociativa (¿Por qué es evidente?).
    \begin{remark}
        Solo para comparar, si hubiésemos hecho esta última demostración utilizando la fórmula clásica del producto matricial, hubieramos tenido que probar la siguiente identidad de sumatorias dobles:
        \[\sum_{k=1}^m C_{kj}\qty(\sum_{l=1}^p A_{il}B_{lk}) = \sum_{l=1}^p A_{il} \qty(\sum_{k=1}^m B_{lk}C_{kj})\]
    \end{remark}

\end{enumerate}