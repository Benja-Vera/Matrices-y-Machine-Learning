% SGD vs BFGS-l
@inproceedings{optimizacion-methods-for-dl,
author = {Le, Quoc V. and Ngiam, Jiquan and Coates, Adam and Lahiri, Abhik and Prochnow, Bobby and Ng, Andrew Y.},
title = {On Optimization Methods for Deep Learning},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69\% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {265–272},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

% Apunte mate 1
@misc{apunte-mate1,
  author        = {González, Alejandro and Ojeda, Harold and Morales, Iván},
  title         = {Apunte de matemáticas 1},
  month         = {3},
  year          = {2023},
  publisher={Programa Académico de Bachillerato, Universidad de Chile}
}

% Spivak
@book{spivak1988cálculo,
  title={Cálculo Infinitesimal},
  author={Spivak, M. and Marqués, B.F.},
  isbn={9788429151367},
  url={https://books.google.cl/books?id=mjdXY8rshREC},
  year={1988},
  publisher={Reverté}
}

% Dummit
@book{dummit2003abstract,
  title={Abstract Algebra},
  author={Dummit, D.S. and Foote, R.M.},
  isbn={9780471433347},
  lccn={2003057652},
  url={https://books.google.cl/books?id=KJDBQgAACAAJ},
  year={2003},
  publisher={Wiley}
}

% Hoffman
@book{HoffmanLA,
  added-at = {2017-06-29T07:13:07.000+0200},
  author = {Hoffman, Kenneth and Kunze, Ray A.},
  biburl = {https://www.bibsonomy.org/bibtex/2b53e33fcf33c53dc977c4c443fe67c26/gdmcbain},
  citeulike-article-id = {13939141},
  citeulike-linkout-0 = {http://www.worldcat.org/isbn/8120302702},
  citeulike-linkout-1 = {http://books.google.com/books?vid=ISBN8120302702},
  citeulike-linkout-2 = {http://www.amazon.com/gp/search?keywords=8120302702\&index=books\&linkCode=qs},
  citeulike-linkout-3 = {http://www.librarything.com/isbn/8120302702},
  citeulike-linkout-4 = {http://www.worldcat.org/oclc/695294341},
  edition = {Second},
  interhash = {cfa84e1c73c53643b8e08f90817198df},
  intrahash = {b53e33fcf33c53dc977c4c443fe67c26},
  isbn = {8120302702},
  keywords = {15-01-linear-and-multilinear-algebra-matrix-theory-instructional-exposition},
  posted-at = {2016-02-23 22:22:09},
  priority = {2},
  publisher = {PHI Learning},
  timestamp = {2017-06-29T07:13:07.000+0200},
  title = {{Linear Algebra}},
  url = {http://www.worldcat.org/isbn/8120302702},
  year = 2004
}

% Kolmogorov
@book{Kolmogorov,
  author = {Kolmogorov, A.N. and Fomin, S.V},
  edition = {First},
  publisher = {Editorial MIR},
  title = {{Elementos de la Teoría de Funciones y del Análisis Funcional}},
  year = 1962
}